\chapter{Design and Implementation} \label{chap:impl}

The primary deliverable of this dissertation is a system (shown in Figure \ref{fig:overview}) that is able to generate metabolic models of cell-free systems.
Our system has four major parts.
First, we ingest experimental data and incorporate it into a standardized format.
Next, we convert the data into a group of cellular metabolic models and sample fluxes from those models to create a dataset .
Then, we perform dimensionality reduction to elucidate underlying trends in the experimental data.
Finally, we use those insights to reduce the original, overspecified models to standalone cell-free metabolic models.

\section{Data gathering}
In order to build a robust model that reflected biological reality, we first had to generate the data we wanted to use for training.
Gathering high quality biological data is difficult and was crucial key to the success of the system as a whole.
We describe the high level process of creating a cell-free system and running the experiments below.
The full experimental protocol can be found in Appendix \ref{app:exp}.

\subsection{Cell-free systems}
As shown in Fig \ref{fig:cfps}, creating a cell-free protein expression system involves three main steps: growing and lysing cells, supplementing with energy substrates, and adding custom DNA constructs.
Our protocol is based off of the open protocol out of the Federici lab \cite{medina2017cfps}.
We begin by growing up 1L of BL21 E. coli cells in an overnight culture of LB until they have reached OD of 1.6.
Then, we spin down the cells and remove the supernatant, storing the pellet in the refrigerator overnight if necessary.
Next, we perform 3 more sets of spins and washes with S30 buffers to remove any extracellular debris.
Finally, we use a bead beater to lyse the cells and spin one last time to remove the cellular debris and beads.
At the end of this process we have cell extract which can be stored in a -80\degree freezer.
\begin{figure}[t!]
\begin{center}
\includegraphics{figs/CellFreeSetup.pdf}
\caption{Overall process for creating a CFPS}
\end{center}
\label{fig:cfps}
\end{figure}

Once we have this cell extract, we have the core cellular machinery that is necessary for transcription and translation.
However, we need to add the cofactors and reactants that are important for the transcription and translation reactions.
So, we add energy substrates to the cell extract to create a cell-free protein expression system.
These substrates include energy substrates such as cAMP, maltodextrin, and NAD.
They also include vital parts of transcription and translation such as NTPs, AAs, and tRNAs.
See Table \ref{tab:cf-nrg} for a complete listing of the reactants and their purpose in the system.
Table \ref{tab:cf-conc} shows the final concentration for each substrate.

\begin{table}[]
\centering
\caption{List of reactants for a 50 $\mu L$ CFPS reaction. 
Note that MDX is a mixture of substrates and that AAs include all 20 of the amino acids.}
\label{tab:cf-nrg}
\begin{tabular}{lll}
Reactant     & Amount ($\mu L$) & Purpose                         \\
MDX          & 5               & Energy substrates               \\
AAs          & 10              & Building blocks for translation \\
PEG          & 2.5             & Molecular crowding              \\
HMP          & 1               & Phosphate source                \\
Mg           & 0.74            & Important cofactor              \\
K            & 0.8             & Charge homeostasis              \\
DNA          & 0.8             & Template for protein production  \\
Cell Extract & 25              & TX/TL machinery                 \\ \hline
CFPS         & 50              & Protein production             
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Final concentrations of reactants in our CFPS reaction}
\label{tab:cf-conc}
\begin{tabular}{ll}
Compound     & Concentration (mM) \\
pi           & 0.981              \\
mg           & 5.92               \\
k            & 48.0               \\
nad          & 0.454              \\
coa          & 0.353              \\
camp         & 1.01               \\
folinic acid & 0.092              \\
spermidine   & 0.181              \\
glucose      & 5.42               \\
ATP/GTP      & 2.09               \\
CTP/UTP      & 1.26               \\
amino acids  & 35.6               \\
trna         & 0.010              \\
dna          & 0.071             
\end{tabular}
\end{table}

Given this cell-free expression system, we have essentially assembled a biological computer.
Whatever DNA "program" is added, the system will work to produce the appropriate protein result.
This platform is incredibly powerful because it allows us to easily insert these DNA programs and see results within a few hours.
A typical biological timeline would take far longer because living cells would have to be coerced to uptake the DNA and incorporate it into their production process.

%TODO talk about debugging/contamination issue

\subsection{Datasets}
We used the protocol above to create two datasets for training.
The first dataset was created by hand and followed standard lab procedures.
We pipetted each of the reactants in the ratios specified by Table \ref{tab:cf-nrg} to create a 200 uL mastermix. 
From that mastermix, we then used 5 uL aliquots combined with each of our different reaction conditions.
In order to test different reaction conditions, we added an additional 1 uL of various reactants.
For this dataset, we experimented by adding 16 differing ratios of sugar, phosphate, potassium, and nucleotides.
The DNA circuit in the CFPS system simply produces RFP, so we were able to judge the amount of protein production by measuring fluorescence readout with a plate reader..

However, there are multiple downsides to doing this by hand, so our second dataset was created using a Labcyte Echo acoustic liquid handler~\cite{TODO}.
Creating large datasets by hand takes a long time and limits the amount of data one can generate.
Additionally, pipetting by hand has an intrinsic error, so we hoped that automating the creation of the reactions would reduce human error.
The Echo is an acoustic liquid handler that can combine different amounts of liquid to create each of the CFPS reactions.
We were able to perform the experiments at 2uL of CFPS system and 500 nL of additional reactants.
The use of automation allowed us to test 48 different reaction conditions and shows that this could be grown to an even larger scale.

Finally, we received a third dataset from the recent Karim and Jewett paper that uses CFPS systems to perform metabolic engineering~\cite{karim2018controlling}.
Our protocol is very similar to the one used in the Jewett lab, though some of the reactant concentrations differ.
This dataset is primarily useful because it was created in a different lab and they investigated a metabolic pathway whose output is not fluorescence.
Since it was created in different lab conditions, we can check that our model is learning something about cell-free systems in general, not just overfitting our data.
The pathway that they investigated is an inherently metabolic pathway, so we were can see if our metabolic model is better able to learn about these types of gene circuits.

\section{Data ingestion and incorporation}
2 csvs
1 for experimental setup
2 columns, one with canonical abbreviation of chemical, other with end concentration
This data then gets converted into fluxes and put in as a constraining exchange reaction

\subsection{Data ingestion}
Since this system is intended to aid biologists in their experiments, we wanted to make it end-to-end and as easy to use as possible.
With that in mind, we created tools to automatically incorporate the experimental setup into the models.
There are two main ways that the experimental setup could show up.
The first is in the form of end concentrations of the different reactants in the cell-free system.
This is the easiest because we are able to convert concentrations into fluxes.
Thus, our tool is able to ingest that in the form of a CSV that has a column with the name of the reactant and the final concentration.

However, we also support biologists who just know the relative amounts of each reactant they add to the mixture without knowing the final concentration.
Thus, we support a format whereby a user can upload a CSV that contains the name of each reactant, the molecular weight, the volume used to create the stock, the weight used to create the stock, and the final volume added to the cell-free reaction.
From that, we are able to calculate the final concentrations and manipulate the data into the same form as the first type of upload.
Once we have that, we can unify the pipeline.

\begin{figure}[t!]
\begin{center}
\includegraphics{figs/DataIngestion.pdf}
\caption{}
\end{center}
\label{fig:ingest}
\end{figure}

\subsection{Data Incorporation} \label{sec:incorp}
First, we need to make the cell-free reaction reflect the fact that there are no longer any membranes or cellular compartments.
We do this by iterating over every reaction that contains a periplasmic or extracellular component.
For each reaction, we then replace every metabolite with either the corresponding cytosolic metabolite or, if no corresponding metabolite exists, we just move the metabolite wholesale.
Thus, at the end, we have all of the reactions occurring in the same location, which reflects biological reality.

Our next task is to remove exchanges and replace the medium with our data.
We do this by using the concentrations we calculated from the setup data earlier.
For each of the reactants that we have, we remove the previous exchange reactions.
Then, we convert the final concentration to a flux using this equation: TODO.
Finally, we add back in the exchange reactions with that flux as an upper bound.
This represents the fact that the metabolite could be entirely used up, but it doesn't have to be used at that level.
At the end of this, we have a base cell-free model.

We can use this model as a basis, but we can also extend this model to explicitly incorporate the transcription and translation reactions we care about.
Given the sequence of the gene of interest, we automatically generate the hand-crafted cell-free FBA model from the Varner lab in Julia~\cite{vilkhovoy2017sequence}.
We can then convert that model to python and extract the transcription and translation reactions.
We incorporate those reactions into our model and then can set the production of the protein of interest as our FBA objective.

Once we've done this, we need to create differential models based on the different experimental data setups.
To do this, we take in a CSV consisting of the experimental conditions and a quantitative output.
Each row represents a different experiment, while the columns represent different starting conditions.
We can then re-calculate the final concentrations of each of the reactants for each experimental starting condition and create a modified FBA model for each starting conditions.


%As a linear programming problem, we also need to set a custom objective to optimize for.
%Usually this is the biomass objective, which encapsulates everything necessary for bacterial growth.
%However, since we don't care about growth, just production, we can use our own objectives.
%For us, the typical objective will be the production of some protein since that's the purpose of the CFPS.

\section{FBA and flux sampling}

\subsection{FBA}

Were we simply to solve the different models we've created, many would have the same flux results.
When we do solve these models without performing dimensionality reduction, we get a correlation of TODO (nearly 0) with our experimental results.
Clearly, FBA alone cannot describe the full richness of a real system.
One can imagine FBA as a coarse bijective function, so points that vary only slightly in the input space will get projected into the same output point.
We can do better by first passing the fluxes through a dimensionality reduction technique such as a VAE.

We also have the problem that the model is overspecified.
These FBA models contain every reaction that's occurring in a steady state bacterium
While we do harvest the bacteria at steady state, some enzymes will degrade and others will not be as important in cell-free systems.
These types of changes can't be encapsulated solely through a change of objective function.

If we just run the naive FBA models on the different reaction conditions, we find that we have a correlation of almost 0.
This means that the typical FBA model just doesn't describe CFPS
So, we want to use a VAE to improve the way we describe CFPS.
However, we first need a large dataset to train on.
The way we accomplish this is by flux sampling from each model.

\subsection{Flux sampling}
In order to determine possible fluxes through each reaction, need to sample from it
We use OptGP sampler to sample 2000 fluxes from each of our models.
Can see that increasing the number of samples leads to a more normal distribution
Figure \ref{fig:fl-samps} shows what happens as we increase the number of samples we draw from each model.

We sample from our different reactions to generate a new dataset.
Each experimental condition has slightly different flux distributions for each reaction.
This is important because it's these minor perturbation that we hope to use to create a specified model.
We then combine these sampled fluxes into a dataset of size N x E x R where N is the number of samples, E is the number of experiments, and R is the number of reactions.

\section{VAE}
We next implemented a VAE in order to examine the latent space of the fluxes.
Our original implementation simply took in the flux dataset as a $n x D$ matrix X with a vector $n x 1$ y that specified the "class" of each flux (i.e. which model it came from).
We then ran it through a 2 layer autoencoder with layer sizes TODO and TODO.
We experimented with latent dimensions of size 2 and 10.
This created a latent space of size $n x 2$.

We were able to explore this latent space.
However, this only reflects information about the fluxes.
We wanted to incorporate the experimental data we'd gathered--we know which models should be producing more fluxes than the others.
We used this insight to create a new loss function for our autoencoder.

\section{Correlated VAEs}
We have additional information that we can use to perturb our latent space.
We know what the relative rankings of the different models should be.
So, instead of just having the typical loss function of an autoencoder: reconstruction loss and KL divergence, we add another term that we call correlation loss.
We are forced to use pearson correlation loss because we can write that in a differentiable manner.

\section{FBA model reduction}
Finally, now that we have run our models through the Corr-VAE and achieved a latent representation, we need to use the insights we've generated to create better models.
We can do this in two ways.
Firstly, we could do this by any time we want to generate new data, we first generate the FBA model using the tool chain described above.
Then, using the trained autoencoder, we can transform the optimal fluxes and get the shifted optimum and see if that still is better.
This could be very useful for batch variation.

However, it would also be nice to not have to run it through an autoencoder each time.
Instead, we could use the insights we've gained to actually create a more accurate reduced model.
We do this by thresholding the latent space and removing reactions that have fluxes similar to 0.
This allows us to create more useful reduced models.
Now, running these reduced models with the differential conditions gives us a correlation of up to 0.5.

\cite{ebrahim2013cobrapy}