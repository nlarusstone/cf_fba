{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Lambda\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from tensorflow.contrib.metrics import streaming_pearson_correlation\n",
    "from keras.models import load_model\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import maxabs_scale, minmax_scale, normalize, scale, robust_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "if not '/home/nlarusstone/cf_fba' in sys.path:\n",
    "    sys.path.append('/home/nlarusstone/cf_fba')\n",
    "import src.utils as utils\n",
    "import src.flux_utils as futils\n",
    "import src.create_dataset as dataset\n",
    "import src.flux_sample as fs\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fs.get_exp_data('../data/{0}_data.CSV')\n",
    "n_experiments = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = futils.scale_data(data=df['AVG.1'].values, scale_type='flux_zero', in_place=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, obj_col, cols = futils.read_data('../data/f2000', 'karim_karim_ecoli_cf_base.sbml_fluxes',\n",
    "                                                                    n_experiments, 'DM_btol_c', n_rows=50000, scale='flux_zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froot = 'hand'\n",
    "txtl = False\n",
    "resamp = True\n",
    "fname = '../data/{0}{1}_{2}_fluxes'.format(froot, '_txtl' if txtl else '', 'stacked' if resamp else 'flat')\n",
    "X_train, y_train, X_test, y_test, obj_col, cols, y_vals_d = dataset.get_dataset(fname)\n",
    "y_vals = np.array(y_vals_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed('../data/fluxes_resampled', train=X_train, test=X_test)\n",
    "#dat = np.load('../data/fluxes_resampled.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "epsilon_std = 1.0\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], n_experiments, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "def corr_loss_np(y_true, y_pred):\n",
    "    cent_pred = y_pred - np.mean(y_pred)\n",
    "    cent_tr = y_true - np.mean(y_true)\n",
    "\n",
    "    std_pred = np.std(y_pred)\n",
    "    std_tr = np.std(y_true)\n",
    "\n",
    "    return np.mean(cent_pred*cent_tr)/(std_pred*std_tr)\n",
    "\n",
    "def corr_loss(y_true, y_pred):\n",
    "    cent_pred = y_pred - K.mean(y_pred)\n",
    "    cent_tr = y_true - K.mean(y_true)\n",
    "\n",
    "    std_pred = K.std(y_pred)\n",
    "    std_tr = K.std(y_true)\n",
    "    return K.mean(cent_pred*cent_tr)/(std_pred*std_tr)\n",
    "\n",
    "# y_true, y_pred\n",
    "def gen_vae_loss(y_true, x_decoded_mean, z_log_var, z_mean):\n",
    "    output_flux = x_decoded_mean[:, :, output_ind]\n",
    "    #experiment_loss = scipy.stats.spearmanr(targets, output_flux)\n",
    "    #experiment_loss = pearson_corr(output_flux, targets)#streaming_pearson_correlation(output_flux, targets)\n",
    "    xent_loss = X_shape * metrics.mean_squared_error(y_true, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)# + experiment_loss\n",
    "    return vae_loss\n",
    "\n",
    "def kl_loss(z_log_var, z_mean):\n",
    "    return - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "def build_vae(X_shape, n_experiments, targets, output_ind, corr_impt, batch_size=100):\n",
    "    encoded_dim1 = 1024\n",
    "    encoded_sz = 256\n",
    "    # Encoder network\n",
    "    x = Input(shape=(n_experiments, X_shape,))\n",
    "    #h = Dense(encoded_dim1, activation='relu')(x)\n",
    "    h = Dense(encoded_sz, activation='relu')(x)#h)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(h)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(h)\n",
    "    \n",
    "    # Sample points from latent space\n",
    "    z = Lambda(sampling, output_shape=(n_experiments,latent_dim,))([z_mean, z_log_var])\n",
    "    \n",
    "    # Decoder network\n",
    "    decoder_h = Dense(encoded_sz, activation='relu')\n",
    "    #decoder_h2 = Dense(encoded_dim1, activation='relu')\n",
    "    decoder_mean = Dense(X_shape, activation='tanh')\n",
    "    h_decoded = decoder_h(z)\n",
    "    #h_decoded2 = decoder_h2(h_decoded)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)#2)\n",
    "\n",
    "    # end-to-end autoencoder\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "    #vae = Model(x, [x_decoded_mean, x_decoded_mean])\n",
    "    output_flux = x_decoded_mean[:, :, output_ind]\n",
    "    #experiment_loss = scipy.stats.spearmanr(targets, output_flux)\n",
    "    experiment_loss_val = -1 * corr_loss(targets, output_flux)#streaming_pearson_correlation(output_flux, targets)\n",
    "    xent_loss = x.shape[-1].value * metrics.mean_squared_error(x, x_decoded_mean)\n",
    "    kl_loss_val = kl_loss(z_log_var, z_mean)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss_val) + corr_impt * experiment_loss_val\n",
    "    #print x.shape, x_decoded_mean.shape, z_mean.shape, z_log_var.shape\n",
    "    #print xent_loss.shape, kl_loss_val.shape, experiment_loss_val.shape, vae_loss.shape\n",
    "    #vae_loss = K.sum(vae_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    #vae.compile(optimizer='rmsprop', loss=[lambda x, x_pred: gen_vae_loss(x, x_pred, z_log_var, z_mean),\n",
    "    #                                       lambda x, x_pred: corr_loss(targets, x_pred[:, :, output_ind])])\n",
    "    vae.summary()\n",
    "\n",
    "    # encoder, from inputs to latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "\n",
    "    # generator, from latent space to reconstructed inputs\n",
    "    decoder_input = Input(shape=(n_experiments, latent_dim,))\n",
    "    _h_decoded = decoder_h(decoder_input)\n",
    "    #_h_decoded2 = decoder_h2(_h_decoded)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)#2)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "    return vae, encoder, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        self.recon_losses = []\n",
    "        self.kl_losses = []\n",
    "        self.corr_losses = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_true = self.validation_data[0]\n",
    "        y_pred = self.model.predict(self.validation_data[0])\n",
    "        corr_loss = -1 * corr_loss_np(y_vals, y_pred[:, :, obj_col.value])\n",
    "        xent_loss = y_true.shape[-1] * np.mean(np.square(y_true - y_pred), axis=-1)\n",
    "        inputs = [K.learning_phase()] + self.model.inputs\n",
    "        zvar = K.function(inputs=inputs, outputs=[self.model.get_layer('z_log_var').output])\n",
    "        zmn = K.function(inputs=inputs, outputs=[self.model.get_layer('z_mean').output])\n",
    "        z_log_var = zvar([0, self.validation_data[0]])[0]\n",
    "        z_mean = zmn([0, self.validation_data[0]])[0]\n",
    "        kl_loss = - 0.5 * np.sum(1 + z_log_var - np.square(z_mean) - np.exp(z_log_var), axis=-1)\n",
    "        print \"Reconstruction loss: {0}\".format(np.mean(xent_loss))\n",
    "        print \"KL loss: {0}\".format(np.mean(kl_loss))\n",
    "        print \"Corr loss: {0}\".format(corr_loss)\n",
    "        self.recon_losses.append(np.mean(xent_loss))\n",
    "        self.kl_losses.append(np.mean(kl_loss))\n",
    "        self.corr_losses.append(corr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%debug\n",
    "batch_size = 256\n",
    "X_shape, n_experiments = X_train.shape[2], len(y_vals)\n",
    "targets = tf.convert_to_tensor(y_vals, dtype=tf.float32)\n",
    "corr_impt = 2\n",
    "vae, encoder, generator = build_vae(X_shape, n_experiments, targets, obj_col.value, corr_impt, batch_size)\n",
    "es = EarlyStopping(patience=5)\n",
    "lh = LossHistory()\n",
    "#with tf.Session(config=tf.ConfigProto(\n",
    "#                    intra_op_parallelism_threads=32)) as sess:\n",
    "#    K.set_session(sess)\n",
    "hist = vae.fit(X_train,\n",
    "        shuffle='batch',\n",
    "        epochs=20,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, None),\n",
    "        callbacks=[es, lh])\n",
    "#encoder.save('encoder_{0}.h5'.format(scale))\n",
    "#generator.save('generator{0}.h5'.format(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/vae.py -d 2 -b 256 -n 100 --layers 1024 1024 1024 --resamp --no-txtl -f karim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh.corr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hi2', 'r') as f:\n",
    "    b = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, btol_col, cols = futils.read_data('../data/flux_samps_2k', scale='flux_max_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hi2', 'w') as f:\n",
    "    pickle.dump(file=f, obj={'recon_losses': lh.recon_losses, 'kl_losses': lh.kl_losses, 'corr_losses': lh.corr_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(X_test, batch_size=batch_size)\n",
    "x_test_gen = generator.predict(x_test_encoded, batch_size=batch_size)\n",
    "#x_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_corr(gen_fluxes, df, btol_col):\n",
    "    corrs = []\n",
    "    for i in range(gen_fluxes.shape[0]):\n",
    "        corr = scipy.stats.pearsonr(gen_fluxes[i, :, btol_col], df['AVG.1'])\n",
    "        corrs.append(corr)\n",
    "    mn = np.mean(corrs, axis=0)\n",
    "    print mn\n",
    "    print corrs[:5]\n",
    "    return mn[0]\n",
    "check_corr(x_test_gen, df, btol_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rct(df, rct, y_test):\n",
    "    y_new = []\n",
    "    for ind in y_test:\n",
    "        y_new.append(df[rct][ind])\n",
    "    return y_new\n",
    "#get_rct(df, 'Glucose', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cm1 = cm.get_cmap('tab20b', 20)\n",
    "#cm2 = cm.get_cmap('tab20c', 20)\n",
    "cmap = cm.get_cmap('plasma', 41)\n",
    "#cmap = lambda x: cm1(x) if x < 21 else cm2(x)\n",
    "#for j in range(41):\n",
    "j = 0\n",
    "xmin, xmax = np.amin(x_test_encoded[:, j, 0]), np.amax(x_test_encoded[:, j, 0])\n",
    "ymin, ymax = np.amin(x_test_encoded[:, j, 1]), np.amax(x_test_encoded[:, j, 1])\n",
    "x_diff = (xmax - xmin) / 10.0\n",
    "y_diff = (ymax - ymin) / 10.0\n",
    "for col in df.columns[4:]:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(x_test_encoded[:, j, 0], x_test_encoded[:, j, 1], c=get_rct(df, col, y_test), cmap=cmap)\n",
    "    plt.xlim((xmin - x_diff, xmax + x_diff))\n",
    "    plt.ylim((ymin - y_diff, ymax + y_diff))\n",
    "    plt.title(col)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_test_encoded[:, j, 0], x_test_encoded[:, j, 1], c=y_test, cmap=cmap)\n",
    "plt.xlim((xmin - x_diff, xmax + x_diff))\n",
    "plt.ylim((ymin - y_diff, ymax + y_diff))\n",
    "plt.title('Variant')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dim_1, x_test_dim_2 = np.mean(x_test_encoded, axis=1)[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cm1 = cm.get_cmap('tab20b', 20)\n",
    "#cm2 = cm.get_cmap('tab20c', 20)\n",
    "cmap = cm.get_cmap('plasma', 41)\n",
    "#cmap = lambda x: cm1(x) if x < 21 else cm2(x)\n",
    "#for j in range(41):\n",
    "x_test_encoded_agg = np.mean(x_test_encoded, axis=1)\n",
    "x_test_dim_1, x_test_dim_2 = x_test_encoded_agg[:, 0], x_test_encoded_agg[:, 1]\n",
    "xmin, xmax = np.amin(x_test_dim_1), np.amax(x_test_dim_2)\n",
    "ymin, ymax = np.amin(x_test_dim_1), np.amax(x_test_dim_2)\n",
    "x_diff = (xmax - xmin) / 10.0\n",
    "y_diff = (ymax - ymin) / 10.0\n",
    "for col in df.columns[4:]:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(x_test_dim_1, x_test_dim_2, c=get_rct(df, col, y_test), cmap=cmap)\n",
    "    plt.xlim((xmin - x_diff, xmax + x_diff))\n",
    "    plt.ylim((ymin - y_diff, ymax + y_diff))\n",
    "    plt.title(col)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_trials = np.std(flat_data, axis=1)\n",
    "std_fluxes = np.std(flat_data, axis=0)\n",
    "plt.hist(std_trials, label='Experiments')\n",
    "plt.title('STD Devs across experiments')\n",
    "plt.hist(std_fluxes, label='Fluxes')\n",
    "plt.title('STD Devs across fluxes')\n",
    "plt.xlabel('Std dev')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sor_flux = np.argsort(std_fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sor_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_corr(biased_resamp_data, df, btol_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = load_model('../models/encoder_epochs=300_batch=256_dimension=2_corr=True.h5')\n",
    "gen = load_model('../models/generator_epochs=300_batch=256_dimension=2_corr=True.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_bad = load_model('../models/encoder_epochs=100_batch=256_dimension=2_corr=False.h5')\n",
    "gen_bad = load_model('../models/generator_epochs=100_batch=256_dimension=2_corr=False.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_good = load_model('../models/encoder_epochs=100_batch=256_dimension=10_corr=True.h5')\n",
    "gen_good = load_model('../models/generator_epochs=100_batch=256_dimension=10_corr=True.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(biased_resamp_data, enc, gen):\n",
    "    encoded_biased = enc.predict(biased_resamp_data)\n",
    "    decoded_biased = gen.predict(encoded_biased)\n",
    "    return check_corr(decoded_biased, df, btol_col)\n",
    "pred(biased_resamp_data, enc, gen)\n",
    "pred(biased_resamp_data, enc2, gen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(biased_resamp_data, enc, gen):\n",
    "    noise_arr = np.logspace(start=-5, stop=1, num=10)\n",
    "    corrs = []\n",
    "    for noise in noise_arr:\n",
    "        noisy_data = biased_resamp_data.copy()\n",
    "        for i in range(n_experiments):\n",
    "            s = noisy_data[:, i, :].shape\n",
    "            noisy_data[:, i, :] += np.random.normal(scale=noise, size=s)\n",
    "            #noisy_data[:, i, :] = minmax_scale(noisy_data[:, i, :])\n",
    "        #scaled_noisy_data = scale_by_flux(noisy_data)\n",
    "        corr = pred(noisy_data, enc, gen)\n",
    "        corrs.append(corr)\n",
    "    return zip(noise_arr, corrs) + [(0, pred(biased_resamp_data, enc, gen))]\n",
    "#noise_res = add_noise(biased_resamp_data, enc, gen)\n",
    "#noise_res2 = add_noise(biased_resamp_data, enc_good, gen_good)\n",
    "noise_res3 = add_noise(biased_resamp_data, enc_bad, gen_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_corr = check_corr(biased_resamp_data, df, btol_col)\n",
    "def plt_noise_corr(noise_data, orig_corr, ndim=2):\n",
    "    orig_enc, noise_data = noise_data[-1], noise_data[:-1]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title('Latent dimension = {0}'.format(ndim))\n",
    "    plt.axhline(y=-1 * orig_enc[1] if orig_enc[1] < 0 else orig_enc[1], label='Original data encoded')\n",
    "    plt.axhline(y=orig_corr, label='Original data correlation', c='g')\n",
    "    for noise, corr in noise_data:\n",
    "        plt.scatter(x=noise, y=-1 * corr if corr < 0 else corr)\n",
    "        plt.xlabel('Noise amount')\n",
    "        plt.ylabel('Correlation')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plt_noise_corr(noise_res, orig_corr, ndim=2)\n",
    "plt_noise_corr(noise_res2, orig_corr, ndim=10)\n",
    "plt_noise_corr(noise_res3, orig_corr, ndim='2, no correlation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc = enc.predict(X_test)\n",
    "test_dec = gen.predict(test_enc)\n",
    "dec_df = pd.DataFrame(data=test_dec[:, 0, :], columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dec_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = cols[dec_df.mean() < 0.001]\n",
    "bad_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "%load_ext autoreload\n",
    "import sys\n",
    "if not '/home/nlarusstone/cf_fba' in sys.path:\n",
    "    sys.path.append('/home/nlarusstone/cf_fba')\n",
    "import src.utils as utils\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cobra.io.read_sbml_model('../models/ecoli_cf_base.sbml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objs = []\n",
    "for i, row in df.iterrows():\n",
    "    print i\n",
    "    model_i = utils.add_reagents_to_model(model, row)\n",
    "    sol = model_i.optimize()\n",
    "    objs.append(sol.objective_value)\n",
    "scipy.stats.pearsonr(objs, df['AVG.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh = np.logspace(start=-4, stop=0, num=20)\n",
    "for t in thresh:\n",
    "    objs = []\n",
    "    for i, row in df.iterrows():\n",
    "        model_i = utils.add_reagents_to_model(model, row)\n",
    "        dec_df = pd.DataFrame(data=test_dec[:, i, :], columns=cols)\n",
    "        bad_cols = cols[dec_df.mean() < t]\n",
    "        model_i.remove_reactions(bad_cols)\n",
    "        sol = model_i.optimize()\n",
    "        objs.append(sol.objective_value)\n",
    "    print t, scipy.stats.pearsonr(objs, df['AVG.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(objs, df['AVG.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objs = []\n",
    "for i, row in df.iterrows():\n",
    "    print i\n",
    "    model_i = utils.add_reagents_to_model(model, row)\n",
    "    dec_df_g = pd.DataFrame(data=test_dec_good[:, i, :], columns=cols)\n",
    "    bad_cols = cols[dec_df_g.mean() < 0.01]\n",
    "    print len(bad_cols)\n",
    "    model_i.remove_reactions(bad_cols)\n",
    "    sol = model_i.optimize()\n",
    "    objs.append(sol.objective_value)\n",
    "scipy.stats.pearsonr(objs, df['AVG.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_good = enc_good.predict(X_test)\n",
    "test_dec_good = gen_good.predict(test_enc)\n",
    "dec_df_good = pd.DataFrame(data=test_dec[:, 0, :], columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols_2 = cols[dec_df_good.mean() < 0.001]\n",
    "bad_cols_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols_2.isin(bad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
