{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import cobra\n",
    "import cobra.test\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "if not '/Users/nlarusstone/Documents/MPhil/Research/cf_fba' in sys.path:\n",
    "    sys.path.append('/Users/nlarusstone/Documents/MPhil/Research/cf_fba')\n",
    "import src.utils as utils\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = cobra.io.load_json_model(filename='../models/ecoli_but.json')\n",
    "model = cobra.io.read_sbml_model('../models/ecoli_cf_base.sbml')\n",
    "#model = cobra.io.read_sbml_model('../models/iJO1366.xml')\n",
    "mod_cf = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Karim_MetEng_2018_Figure2_Data.csv')\n",
    "df.drop(columns=['Area_1', 'Area_2', 'Conc_1', 'Conc_2'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Rand_Env(gym.Env):      \n",
    "    def __init__(self, model, df):\n",
    "        self.model = model.copy()\n",
    "        self.rxns = model.reactions\n",
    "        self.n_rxns = len(self.rxns)\n",
    "        # TODO Add metabs\n",
    "        self.metabs = model.metabolites\n",
    "        n_metabs = len(self.metabs)\n",
    "        # LEARNING FROM EXPERIMENTAL DATA:\n",
    "        self.df = df\n",
    "        self.cond_scores = df['AVG.1']\n",
    "        # Add 1 for not removing any\n",
    "        self.action_space = spaces.MultiBinary(self.n_rxns) # TODO: Think about using MultiBinary\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(n_rxns + 1), \"metabolite\": spaces.Discrete(n_metabs + 1)})\n",
    "        # Whether or not a reaction is present\n",
    "        self.observation_space = spaces.MultiBinary(self.n_rxns)\n",
    "        # In case they forget to reset\n",
    "        self.state = np.ones(self.n_rxns, dtype=np.int8)\n",
    "        self.time = 0\n",
    "        \n",
    "        self.unique_fluxes = {}\n",
    "        \n",
    "        self.log = []\n",
    "        \n",
    "        self._seed()\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(2), \"metabolite\": spaces.Discrete(3)})\n",
    "        \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "    \n",
    "    def _take_action(self, state, action):\n",
    "        # Returns new state\n",
    "        return NotImplemented\n",
    "    \n",
    "    def _evaluate(self, state_rxns):\n",
    "        return NotImplemented\n",
    "    \n",
    "    def reset(self):\n",
    "        # TODO, choose random starting state\n",
    "        self.state = np.ones(self.n_rxns, dtype=np.int8)\n",
    "        self.last_action = None\n",
    "        self.time = 0\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self,action):\n",
    "        # TODO: do better than this?\n",
    "        new_state = self._take_action(self.state, action)\n",
    "        self.state = new_state\n",
    "        reward = self._evaluate(self.state)\n",
    "\n",
    "        done = False\n",
    "        #if self.time > 300:\n",
    "        #    done = True\n",
    "\n",
    "        self.time += 1\n",
    "            \n",
    "        return self.rxns[action[0]], reward, done, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Samp_Env(FBA_Rand_Env):  \n",
    "    def _take_action(self, state, act):\n",
    "        state = act\n",
    "        return state\n",
    "    \n",
    "    def _evaluate(self, state_rxns):\n",
    "        self.model.repair()\n",
    "        # 1 represents a reaction to keep, so remove if not 1\n",
    "        rxns_to_remove = [i for i,j in zip(self.model.reactions, state_rxns) if not j]\n",
    "        with self.model as model:\n",
    "            model.remove_reactions(reactions=rxns_to_remove, remove_orphans=False)\n",
    "            objs = utils.add_addl_reactants(model, self.df)\n",
    "            #objs, fluxes = utils.gen_fluxes_addl_reactants(self.cur_model, self.df)\n",
    "            #obj, fluxes = utils.gen_fluxes(self.cur_model)\n",
    "            #if not tuple(fluxes) in self.unique_fluxes:\n",
    "            #  self.unique_fluxes[tuple(fluxes)] = state_rxns\n",
    "            if not tuple(objs) in self.unique_fluxes:\n",
    "                self.unique_fluxes[tuple(objs)] = state_rxns\n",
    "            if sum(objs) < 0.01:\n",
    "                return -1000\n",
    "            corr = scipy.stats.spearmanr(objs, self.cond_scores)\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done, prev_action):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBackAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done, prev_action):\n",
    "        if reward < -1:\n",
    "            return (prev_action[0], 'add')\n",
    "        return (self.action_space.sample(), 'remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FBA_Samp_Env(mod_cf, df)\n",
    "agent = RandomAgent(env.action_space)\n",
    "max_reward = (0, None)\n",
    "prev_action, done = None, False\n",
    "for i_episode in range(2):\n",
    "    print 'Episode {0}'.format(i_episode)\n",
    "    observation = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(1001):\n",
    "        #env.render()\n",
    "        #print(observation)\n",
    "        action = agent.act(observation, reward, done, prev_action)\n",
    "        observation, reward, done, prev_action = env.step(action)\n",
    "        if np.isnan(reward).any():\n",
    "            print 'Nan', observation\n",
    "            break\n",
    "        if t % 10 == 0:\n",
    "            print 'Time {0}, reward: {1}'.format(t, reward)\n",
    "        if reward > max_reward[0]:\n",
    "            max_reward = (reward, observation)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "print max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_rxns = []\n",
    "i = 0\n",
    "for objs, rxns in env.unique_fluxes.items():\n",
    "    i += 1\n",
    "    if sum(objs) < 0.01:\n",
    "        continue\n",
    "    dis_rxns.append(rxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print i\n",
    "print len(dis_rxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = (0, None)\n",
    "for rxn in range(len(dis_rxns)):\n",
    "    for flux_ser in dis_rxns[rxn][0]:\n",
    "        if flux_ser.shape[0] > max_len[0]:\n",
    "            max_len = (flux_ser.shape[0], flux_ser)\n",
    "ind = max_len[1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes = []\n",
    "for rxn in range(len(dis_rxns)):\n",
    "    experiments = []\n",
    "    for flux_ser in dis_rxns[rxn][0]:\n",
    "        flux_ser_pad = flux_ser.reindex(ind)\n",
    "        experiments.append(np.array(flux_ser_pad))\n",
    "    experiment = np.stack(experiments, axis=0)\n",
    "    fluxes.append(experiment)\n",
    "flux_arr = np.stack(fluxes, axis=0)\n",
    "flux_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_arr[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/fluxes_ecoli_but', flux_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_arr_no_nan = np.nan_to_num(x=flux_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def viz_flux_std(flux_arr):\n",
    "stds = []\n",
    "for exp in flux_arr_no_nan:\n",
    "    for cond in exp:\n",
    "        stds.append(np.std(cond))\n",
    "#viz_flux_std(flux_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Env(gym.Env):      \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.rxns = model.reactions\n",
    "        self.n_rxns = len(self.rxns)\n",
    "        # TODO Add metabs\n",
    "        self.metabs = model.metabolites\n",
    "        n_metabs = len(self.metabs)\n",
    "        # Add 1 for not removing any\n",
    "        self.action_space = spaces.Discrete(n_rxns + 1) # TODO: Think about using MultiBinary\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(n_rxns + 1), \"metabolite\": spaces.Discrete(n_metabs + 1)})\n",
    "        # Whether or not a reaction is present\n",
    "        self.observation_space = spaces.MultiBinary(n_rxns + 1)\n",
    "        self.state = self.observation_space.sample()\n",
    "        self.time = 0\n",
    "        \n",
    "        self._seed()\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(2), \"metabolite\": spaces.Discrete(3)})\n",
    "\n",
    "    def _evaluate(self, state_rxns):\n",
    "        # 1 represents a reaction to keep, so remove if not 1\n",
    "        rxns_to_remove = [i for i,j in zip(self.rxns, state_rxns) if not j]\n",
    "        with self.model as model:\n",
    "            model.remove_reactions(rxns_to_remove)\n",
    "            obj = model.slim_optimize()\n",
    "        return obj\n",
    "        \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def step(self,action):\n",
    "        cur_rxns = self.state        \n",
    "        x = self.state\n",
    "        reward = self._evaluate(cur_rxns)\n",
    "        # TODO: do better than this?\n",
    "        new_rxns = self.observation_space.sample()\n",
    "        new_x = new_rxns\n",
    "\n",
    "        self.state = new_x\n",
    "\n",
    "        done = False\n",
    "        if self.time > 300:\n",
    "            done = True\n",
    "\n",
    "        self.time += 1\n",
    "            \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # self.step(np.array([0,0,0,0]))[0]\n",
    "        # TODO, choose random starting state\n",
    "        self.state = np.ones(self.n_rxns, dtype=np.int8)\n",
    "        self.last_u = None\n",
    "        self.time = 0\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "\n",
    "    #def get_params(self):\n",
    "    #    return self.A, self.B, self.Q, self.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
