{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import cobra\n",
    "import cobra.test\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "if not '/Users/nlarusstone/Documents/MPhil/Research/cf_fba' in sys.path:\n",
    "    sys.path.append('/Users/nlarusstone/Documents/MPhil/Research/cf_fba')\n",
    "import src.utils as utils\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = cobra.io.load_json_model(filename='../models/ecoli_but.json')\n",
    "model = cobra.io.read_sbml_model('../models/ecoli_cf_base.sbml')\n",
    "#model = cobra.io.read_sbml_model('../models/iJO1366.xml')\n",
    "mod_cf = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Karim_MetEng_2018_Figure2_Data.csv')\n",
    "df.drop(columns=['Area_1', 'Area_2', 'Conc_1', 'Conc_2'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Env(gym.Env):      \n",
    "    def __init__(self, model, df):\n",
    "        self.model = model.copy()\n",
    "        self.cur_model = self.model.copy()\n",
    "        self.rxns = model.reactions\n",
    "        self.n_rxns = len(self.rxns)\n",
    "        # TODO Add metabs\n",
    "        self.metabs = model.metabolites\n",
    "        n_metabs = len(self.metabs)\n",
    "        # LEARNING FROM EXPERIMENTAL DATA:\n",
    "        self.df = df\n",
    "        self.cond_scores = df['AVG.1']\n",
    "        # Add 1 for not removing any\n",
    "        self.action_space = spaces.Discrete(self.n_rxns) # TODO: Think about using MultiBinary\n",
    "        self.action_types = ('remove', 'add')\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(n_rxns + 1), \"metabolite\": spaces.Discrete(n_metabs + 1)})\n",
    "        # Whether or not a reaction is present\n",
    "        self.observation_space = spaces.MultiBinary(self.n_rxns)\n",
    "        # In case they forget to reset\n",
    "        self.state = np.ones(self.n_rxns, dtype=np.int8)\n",
    "        self.time = 0\n",
    "        \n",
    "        self.unique_fluxes = {}\n",
    "        \n",
    "        self.log = []\n",
    "        \n",
    "        self._seed()\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(2), \"metabolite\": spaces.Discrete(3)})\n",
    "        \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "    \n",
    "    def _take_action(self, state, action):\n",
    "        # Returns new state\n",
    "        return NotImplemented\n",
    "    \n",
    "    def _evaluate(self, state_rxns):\n",
    "        return NotImplemented\n",
    "    \n",
    "    def reset(self):\n",
    "        # TODO, choose random starting state\n",
    "        self.state = np.ones(self.n_rxns, dtype=np.int8)\n",
    "        self.cur_model = self.model.copy()\n",
    "        self.last_action = None\n",
    "        self.time = 0\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self,action):\n",
    "        # TODO: do better than this?\n",
    "        new_state = self._take_action(self.state, action)\n",
    "        self.state = new_state\n",
    "        reward = self._evaluate(self.state)\n",
    "\n",
    "        done = False\n",
    "        #if self.time > 300:\n",
    "        #    done = True\n",
    "\n",
    "        self.time += 1\n",
    "            \n",
    "        return self.rxns[action[0]], reward, done, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Step_Env(FBA_Env):  \n",
    "    def _take_action(self, state, act):\n",
    "        action, action_type = act\n",
    "        if action_type == 'add':\n",
    "            state[action] = 1\n",
    "            rxn = self.rxns[action]\n",
    "            self.cur_model.add_reaction(rxn)\n",
    "        else:\n",
    "        # Returns new state\n",
    "            state[action] = 0\n",
    "            rxn = self.rxns[action]\n",
    "            if rxn in self.cur_model.reactions:\n",
    "                self.cur_model.reactions.get_by_id(rxn.id).remove_from_model()\n",
    "            else:\n",
    "                self.log.append(('rxn {0} not in model with state {1}'.format(rxn, self.state)))\n",
    "        return state\n",
    "    \n",
    "    def _evaluate(self, state_rxns):\n",
    "        objs = utils.add_addl_reactants(self.cur_model, self.df)\n",
    "        if not tuple(objs) in self.unique_fluxes:\n",
    "            self.unique_fluxes[tuple(objs)] = state_rxns\n",
    "        if sum(objs) < 0.01:\n",
    "            return -1000\n",
    "        corr = scipy.stats.spearmanr(objs, self.cond_scores)\n",
    "        return corr\n",
    "    \n",
    "    def _evaluate_fluxes(self, state_rxns):\n",
    "        #objs = utils.add_addl_reactants(self.cur_model, self.df)\n",
    "        objs, fluxes = utils.gen_fluxes_addl_reactants(self.cur_model, self.df)\n",
    "        if not tuple(objs) in self.unique_fluxes:\n",
    "            self.unique_fluxes[tuple(objs)] = (fluxes, state_rxns)\n",
    "        if sum(objs) < 0.01:\n",
    "            return -1000\n",
    "        corr = scipy.stats.spearmanr(objs, self.cond_scores)\n",
    "        return corr\n",
    "    \n",
    "    def _evaluate_flux(self, state_rxns):\n",
    "        #objs = utils.add_addl_reactants(self.cur_model, self.df)\n",
    "        obj, fluxes = utils.gen_fluxes(self.cur_model)\n",
    "        if not tuple(fluxes) in self.unique_fluxes:\n",
    "            self.unique_fluxes[tuple(fluxes)] = state_rxns\n",
    "        if obj < 10 ** -7:\n",
    "            return -1000\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done, prev_action):\n",
    "        return (self.action_space.sample(), 'remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBackAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done, prev_action):\n",
    "        if reward < -1:\n",
    "            return (prev_action[0], 'add')\n",
    "        return (self.action_space.sample(), 'remove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.flux_analysis.deletion import single_reaction_deletion\n",
    "with mod_cf as model:\n",
    "    print len(model.reactions)\n",
    "    dels = single_reaction_deletion(model=model)\n",
    "    #double_reaction_deletion\n",
    "    #processes = 4\n",
    "    print len(model.reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FBA_Step_Env(mod_cf, df)\n",
    "agent = AddBackAgent(env.action_space)\n",
    "max_reward = (0, None)\n",
    "prev_action, done = None, False\n",
    "for i_episode in range(10):\n",
    "    print 'Episode {0}'.format(i_episode)\n",
    "    observation = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(501):\n",
    "        #env.render()\n",
    "        #print(observation)\n",
    "        action = agent.act(observation, reward, done, prev_action)\n",
    "        observation, reward, done, prev_action = env.step(action)\n",
    "        if np.isnan(reward).any():\n",
    "            print 'NAN'\n",
    "            break\n",
    "        if t % 10 == 0:\n",
    "            print 'Time {0}, reward: {1}'.format(t, reward)\n",
    "        if reward > max_reward[0]:\n",
    "            max_reward = (reward, observation)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "print max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_rxns = []\n",
    "i = 0\n",
    "for objs, rxns in env.unique_fluxes.items():\n",
    "    i += 1\n",
    "    if sum(objs) < 0.01:\n",
    "        continue\n",
    "    dis_rxns.append(rxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print i\n",
    "print len(dis_rxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = (0, None)\n",
    "for rxn in range(len(dis_rxns)):\n",
    "    for flux_ser in dis_rxns[rxn][0]:\n",
    "        if flux_ser.shape[0] > max_len[0]:\n",
    "            max_len = (flux_ser.shape[0], flux_ser)\n",
    "ind = max_len[1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes = []\n",
    "for rxn in range(len(dis_rxns)):\n",
    "    experiments = []\n",
    "    for flux_ser in dis_rxns[rxn][0]:\n",
    "        flux_ser_pad = flux_ser.reindex(ind)\n",
    "        experiments.append(np.array(flux_ser_pad))\n",
    "    experiment = np.stack(experiments, axis=0)\n",
    "    fluxes.append(experiment)\n",
    "flux_arr = np.stack(fluxes, axis=0)\n",
    "flux_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_arr[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/fluxes_ecoli_but', flux_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_arr_no_nan = np.nan_to_num(x=flux_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def viz_flux_std(flux_arr):\n",
    "stds = []\n",
    "for exp in flux_arr_no_nan:\n",
    "    for cond in exp:\n",
    "        stds.append(np.std(cond))\n",
    "#viz_flux_std(flux_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Pathway_Env(FBA_Env):  \n",
    "    def _take_action(self, state, action):\n",
    "        # Returns new state\n",
    "        state[action] = 0\n",
    "        rxn = self.rxns[action]\n",
    "        self.cur_model.reactions.get_by_id(rxn.id).remove_from_model()\n",
    "        return state\n",
    "    \n",
    "    def _evaluate(self, state_rxns):\n",
    "        objs = utils.add_addl_reactants(self.cur_model, self.df)\n",
    "        #for start_cond_dict in self.starting_conds:\n",
    "        #    start_cond = start_cond_dict.keys()\n",
    "            # Update Model with starting condition\n",
    "       #     with self.cur_model as model:\n",
    "        #        model.add_reactions(start_cond)\n",
    "        #        objs.append(model.slim_optimize())\n",
    "        # Discourage infeasible solutions\n",
    "        if sum(objs) == 0:\n",
    "            return -1000\n",
    "        corr = scipy.stats.spearmanr(objs, self.cond_scores)\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mod, starting_conds, cond_scores):\n",
    "    model = mod.copy()\n",
    "    objs = []\n",
    "    for start_cond_dict in starting_conds:\n",
    "        start_cond = start_cond_dict.keys()\n",
    "        # Update Model with starting condition\n",
    "        with model as model:\n",
    "            model.add_reactions(start_cond)\n",
    "            objs.append(model.slim_optimize())\n",
    "    print objs\n",
    "    corr = scipy.stats.spearmanr(objs, cond_scores)\n",
    "    return corr\n",
    "test(mod_cf, rxn_conds, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mod_cf as model:\n",
    "    model.reactions.HSDy.remove_from_model()\n",
    "    model.reactions.UDPGPpp.remove_from_model()\n",
    "    print test(model, rxn_conds, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Learner:\n",
    "\n",
    "    def __init__(self, discount):\n",
    "        self.last_state  = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "        self.Q = {}\n",
    "        self.alphas = {}\n",
    "        self.epsilon = None # will be modified by loop\n",
    "        self.score = 0\n",
    "        self.discount = discount\n",
    "        self.raw_states = []\n",
    "        self.disc_states = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_state  = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "        self.score = 0\n",
    "\n",
    "    def disc_state(self, old_state):\n",
    "        old_bot_diff = old_state['tree']['bot'] - old_state['monkey']['bot']\n",
    "        new_bot_diff = (old_bot_diff + 333) / 100\n",
    "        new_bot_diff = max(0, new_bot_diff)\n",
    "        \n",
    "        old_tree_dist = old_state['tree']['dist']\n",
    "        new_tree_dist = (old_tree_dist + 115) / 120\n",
    "        new_tree_dist = max(0, new_tree_dist)\n",
    "\n",
    "        old_monk_vel = old_state['monkey']['vel']\n",
    "        new_monk_vel = (old_monk_vel + 40) / 20\n",
    "        new_monk_vel = max(0, new_monk_vel)\n",
    "\n",
    "        return (new_bot_diff, new_tree_dist, new_monk_vel)\n",
    "        \n",
    "        \n",
    "    def action_callback(self, state):\n",
    "        '''Implement this function to learn things and take actions.\n",
    "        Return 0 if you don't want to jump and 1 if you do.'''\n",
    "        \n",
    "        self.raw_states.append(state)\n",
    "        self.disc_states.append(self.disc_state(state))\n",
    "\n",
    "        if self.last_state is None:\n",
    "            self.last_state = state\n",
    "            self.last_action = 0\n",
    "            return 0\n",
    "\n",
    "        # self.states.append(state)\n",
    "        discount = self.discount\n",
    "\n",
    "        # UPDATE Q\n",
    "        last_state = self.disc_state(self.last_state)\n",
    "        cur_state = self.disc_state(state)\n",
    "        last_action = self.last_action\n",
    "        last_reward = self.last_reward\n",
    "        \n",
    "        if last_state not in self.Q:\n",
    "            self.Q[last_state] = [0., 0.]\n",
    "            self.alphas[last_state] = [1., 1.]\n",
    "        if cur_state not in self.Q:\n",
    "            self.Q[cur_state] = [0., 0.]\n",
    "            self.alphas[cur_state] = [1., 1.]\n",
    "\n",
    "        old_val = self.Q[last_state][last_action]\n",
    "        alpha = self.alphas[last_state][last_action]\n",
    "        self.Q[last_state][last_action] = old_val + (1./alpha)*(last_reward + discount*max(self.Q[cur_state]) - old_val)\n",
    "        self.alphas[last_state][last_action] += 1.\n",
    "        \n",
    "        # CHOOSE NEW ACTION\n",
    "        rnd = npr.random()\n",
    "\n",
    "        if rnd > self.epsilon:\n",
    "            # choose optimal action\n",
    "            action_vals = self.Q[cur_state]\n",
    "            new_action = 0 if action_vals[0] >= action_vals[1] else 1\n",
    "        else:\n",
    "            # act randomly, 0.7 prob of holding, 0.3 prob of jumping\n",
    "            rnd  = npr.random()\n",
    "            new_action = 0 if rnd < 0.8 else 1\n",
    "\n",
    "        self.last_action = new_action\n",
    "        self.last_state  = state\n",
    "        self.score = state['score']\n",
    "\n",
    "        return self.last_action\n",
    "\n",
    "    def reward_callback(self, reward):\n",
    "        '''This gets called so you can see what reward you get.'''\n",
    "        self.last_reward = reward\n",
    "\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print 'Usage: python QLearn.py numIters discountRate'\n",
    "    sys.exit(0)\n",
    "\n",
    "iters = int(sys.argv[1])\n",
    "discount = float(sys.argv[2])\n",
    "learner = Learner(discount)\n",
    "scores = []\n",
    "\n",
    "for ii in xrange(iters):\n",
    "\n",
    "    learner.epsilon = 1./(ii+1)\n",
    "\n",
    "    # Make a new monkey object.\n",
    "    swing = SwingyMonkey(sound=False,            # Don't play sounds.\n",
    "                         text=\"Epoch %d\" % (ii), # Display the epoch on screen.\n",
    "                         tick_length=1,          # Make game ticks super fast.\n",
    "                         action_callback=learner.action_callback,\n",
    "                         reward_callback=learner.reward_callback)\n",
    "    # Loop until you hit something.\n",
    "    while swing.game_loop():\n",
    "        pass\n",
    "    \n",
    "    scores.append(learner.score)\n",
    "\n",
    "    # Reset the state of the learner.\n",
    "    learner.reset()\n",
    "\n",
    "def moving_average(a, n=10):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.show()\n",
    "plt.plot(moving_average(scores))\n",
    "plt.show()\n",
    "plt.hist(scores)\n",
    "plt.show()\n",
    "print np.median(scores)\n",
    "print np.mean(scores)\n",
    "print max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look into GoalEnv\n",
    "class FBA_Env(gym.Env):      \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.rxns = model.reactions\n",
    "        self.n_rxns = len(self.rxns)\n",
    "        # TODO Add metabs\n",
    "        self.metabs = model.metabolites\n",
    "        n_metabs = len(self.metabs)\n",
    "        # Add 1 for not removing any\n",
    "        self.action_space = spaces.Discrete(n_rxns + 1) # TODO: Think about using MultiBinary\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(n_rxns + 1), \"metabolite\": spaces.Discrete(n_metabs + 1)})\n",
    "        # Whether or not a reaction is present\n",
    "        self.observation_space = spaces.MultiBinary(n_rxns + 1)\n",
    "        self.state = self.observation_space.sample()\n",
    "        self.time = 0\n",
    "        \n",
    "        self._seed()\n",
    "        #spaces.Dict({\"reaction\": spaces.Discrete(2), \"metabolite\": spaces.Discrete(3)})\n",
    "\n",
    "    def _evaluate(self, state_rxns):\n",
    "        # 1 represents a reaction to keep, so remove if not 1\n",
    "        rxns_to_remove = [i for i,j in zip(self.rxns, state_rxns) if not j]\n",
    "        with self.model as model:\n",
    "            model.remove_reactions(rxns_to_remove)\n",
    "            obj = model.slim_optimize()\n",
    "        return obj\n",
    "        \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def step(self,action):\n",
    "        cur_rxns = self.state        \n",
    "        x = self.state\n",
    "        reward = self._evaluate(cur_rxns)\n",
    "        # TODO: do better than this?\n",
    "        new_rxns = self.observation_space.sample()\n",
    "        new_x = new_rxns\n",
    "\n",
    "        self.state = new_x\n",
    "\n",
    "        done = False\n",
    "        if self.time > 300:\n",
    "            done = True\n",
    "\n",
    "        self.time += 1\n",
    "            \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # self.step(np.array([0,0,0,0]))[0]\n",
    "        # TODO, choose random starting state\n",
    "        self.state = np.ones(self.n_rxns, dtype=np.int8)\n",
    "        self.last_u = None\n",
    "        self.time = 0\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "\n",
    "    #def get_params(self):\n",
    "    #    return self.A, self.B, self.Q, self.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
